{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDAE - Stacked Denoising Auto-Encoder\n",
    "\n",
    "Generally, weights in a neural network (for classification) are initialized randomly. This works fine when we have a lot of labelled data. But what if we have a lot of data but only a fraction of it is labelled? A good example of this case is google images. We have a ton of data there, but only a tiny fraction of it is actually labelled. Can we classify all the data by training a network only on that fraction of labelled data?\n",
    "\n",
    "In this experiment we demonstrate that initiaizing the network with pretrained SDAE weights boosts classification performance. In a stacked auto-encoder, every layer is trained individually to predict it's input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 781)\n",
      "(10000, 10)\n",
      "(10000, 781)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries/modules\n",
    "import numpy as np # for array operations\n",
    "from keras.models import Model, Sequential # for defining the architectures\n",
    "from keras.layers import Dense, Dropout, Input # layers for building the network\n",
    "from keras.utils import to_categorical # to_categorical does one-hot encoding\n",
    "\n",
    "tmp = np.load('cifar_pca_train.npz')\n",
    "data = tmp['data']\n",
    "labels = tmp['labels']\n",
    "\n",
    "train_data = data[:10000]      # We'll use only 10000 out of 50000 samples for classification\n",
    "train_labels = labels[:10000]\n",
    "\n",
    "tmp = np.load('cifar_pca_test.npz')\n",
    "test_data = tmp['data']\n",
    "test_labels = tmp['labels']\n",
    "\n",
    "# Converting labels into one-hot vectors for training. one-hot encoding is nothing but dummyfing\n",
    "train_labels = to_categorical(train_labels, 10) \n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an MLP for classification through randomly initialized weights\n",
    "#### Note:  We'll use only 10000 out of 50000 samples for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 12s - loss: 2.1390 - acc: 0.2258 - val_loss: 1.9035 - val_acc: 0.3286\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.9158 - acc: 0.3104 - val_loss: 1.7796 - val_acc: 0.3750\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.8410 - acc: 0.3472 - val_loss: 1.7603 - val_acc: 0.3798\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.7906 - acc: 0.3636 - val_loss: 1.7729 - val_acc: 0.3734\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.7452 - acc: 0.3820 - val_loss: 1.7249 - val_acc: 0.3900\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.7131 - acc: 0.3982 - val_loss: 1.7220 - val_acc: 0.3935\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.6812 - acc: 0.4108 - val_loss: 1.7413 - val_acc: 0.3845\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.6505 - acc: 0.4177 - val_loss: 1.6988 - val_acc: 0.3993\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.6139 - acc: 0.4332 - val_loss: 1.7214 - val_acc: 0.3933\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.5904 - acc: 0.4497 - val_loss: 1.6802 - val_acc: 0.4094\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.5621 - acc: 0.4483 - val_loss: 1.6869 - val_acc: 0.4066\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.5404 - acc: 0.4621 - val_loss: 1.6667 - val_acc: 0.4106\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.5209 - acc: 0.4668 - val_loss: 1.6812 - val_acc: 0.4054\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 14s - loss: 1.4861 - acc: 0.4777 - val_loss: 1.6755 - val_acc: 0.4033\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 15s - loss: 1.4687 - acc: 0.4853 - val_loss: 1.6747 - val_acc: 0.4023\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 14s - loss: 1.4455 - acc: 0.4946 - val_loss: 1.6638 - val_acc: 0.4106\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 14s - loss: 1.4152 - acc: 0.5000 - val_loss: 1.6753 - val_acc: 0.4113\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 14s - loss: 1.4084 - acc: 0.5031 - val_loss: 1.6640 - val_acc: 0.4134\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 14s - loss: 1.3917 - acc: 0.5108 - val_loss: 1.6792 - val_acc: 0.4178\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 14s - loss: 1.3553 - acc: 0.5234 - val_loss: 1.6553 - val_acc: 0.4113\n"
     ]
    }
   ],
   "source": [
    "# training a simple one hidden layer MLP for classification task\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(781,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(800, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "nb_epoch = 20\n",
    "batch_size = 32\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = mlp.fit(train_data, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a two layer stacked autoencoder\n",
    "#### Note:  We'll use all 50000 samples for training. Since no labels are required for autoencoder.\n",
    "\n",
    "The way we do this is: \n",
    "1. Train an AE (autoencoder) with 1000 nodes on the original 781 features. So this will give 1000 encoded features.\n",
    "2. Train another AE (autoencoder1) with 800 nodes on the above 1000 features.\n",
    "\n",
    "The weights of the encoding layers of the above two netwroks can be used to initialize the MLP for classification ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 52s - loss: 0.5822 - val_loss: 0.2333\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 52s - loss: 0.5361 - val_loss: 0.2147\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 52s - loss: 0.5312 - val_loss: 0.2068\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 52s - loss: 0.5286 - val_loss: 0.2022\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 52s - loss: 0.5273 - val_loss: 0.2010\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5246 - val_loss: 0.2013\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5226 - val_loss: 0.1996\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5213 - val_loss: 0.1963\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5196 - val_loss: 0.1994\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5169 - val_loss: 0.1932\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 52s - loss: 0.5181 - val_loss: 0.1941\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5147 - val_loss: 0.1934s: 0.\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5151 - val_loss: 0.1950\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5130 - val_loss: 0.1953\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5104 - val_loss: 0.1959\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5091 - val_loss: 0.1970\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5115 - val_loss: 0.2016\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 55s - loss: 0.5073 - val_loss: 0.2037\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.5072 - val_loss: 0.2046\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.5068 - val_loss: 0.2063\n"
     ]
    }
   ],
   "source": [
    "# Training an autoencoder model on cifar-10 PCA reduced data\n",
    "\n",
    "input_img = Input(shape=(781,))\n",
    "crrpt_img = Dropout(0.5)(input_img)\n",
    "encoded = Dense(1000, activation='sigmoid')(crrpt_img)\n",
    "decoded = Dense(781, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "\n",
    "nb_epoch = 20\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error')\n",
    "\n",
    "history = autoencoder.fit(data, data,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_data, test_data))\n",
    "\n",
    "\n",
    "autoencoder.save('SDAE_l1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_img,encoded)\n",
    "htrain_data = encoder.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.5029    \n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.4836    \n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 53s - loss: 0.4812    \n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.4803    \n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.4799    \n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 55s - loss: 0.4796    \n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 58s - loss: 0.4794    \n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 57s - loss: 0.4792    \n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 56s - loss: 0.4790    \n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 55s - loss: 0.4788    \n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.4787    \n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 55s - loss: 0.4786    \n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.4785    \n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 51s - loss: 0.4784    \n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 51s - loss: 0.4783    \n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.4782    \n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 54s - loss: 0.4781    \n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 55s - loss: 0.4780    \n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 55s - loss: 0.4779    \n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 55s - loss: 0.4779    \n"
     ]
    }
   ],
   "source": [
    "input_img1 = Input(shape=(1000,))\n",
    "crrpt_img1 = Dropout(0.5)(input_img1)\n",
    "encoded1 = Dense(800, activation='sigmoid')(crrpt_img1)\n",
    "decoded1 = Dense(1000, activation='sigmoid')(encoded1)\n",
    "\n",
    "autoencoder1 = Model(input_img1,decoded1)\n",
    "\n",
    "autoencoder1.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy')\n",
    "\n",
    "history = autoencoder1.fit(htrain_data, htrain_data,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True)\n",
    "\n",
    "autoencoder1.save('SDAE_l2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50290534435272216,\n",
       " 0.48362031801223754,\n",
       " 0.4811713487434387,\n",
       " 0.48032720893859865,\n",
       " 0.47986385398864745,\n",
       " 0.47960036675453188,\n",
       " 0.47936453899383547,\n",
       " 0.47915661866188047,\n",
       " 0.47899607859611509,\n",
       " 0.47882437093734742,\n",
       " 0.47872783620834353,\n",
       " 0.47856412561416628,\n",
       " 0.47850920567512512,\n",
       " 0.47841503306388855,\n",
       " 0.4782769642353058,\n",
       " 0.47818892166137694,\n",
       " 0.4781111824798584,\n",
       " 0.47803410091400145,\n",
       " 0.4779452741241455,\n",
       " 0.47785789219856262]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#history.history.keys()\n",
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an MLP for classification through autoencoder initialized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training with unsupervised initialization for layer-1 \n",
    "# of an MLP using autoencoder weights\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(781,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(800, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dropout at 0x7f82edc10940>,\n",
       " <keras.layers.core.Dense at 0x7f82edc10b00>,\n",
       " <keras.layers.core.Dropout at 0x7f82edc10c50>,\n",
       " <keras.layers.core.Dense at 0x7f82edc10dd8>,\n",
       " <keras.layers.core.Dropout at 0x7f82edbc2240>,\n",
       " <keras.layers.core.Dense at 0x7f82edbf4940>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 13s - loss: 2.0550 - acc: 0.2583 - val_loss: 1.7527 - val_acc: 0.3948\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.8333 - acc: 0.3506 - val_loss: 1.6883 - val_acc: 0.4084\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.7409 - acc: 0.3734 - val_loss: 1.6507 - val_acc: 0.4184\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.6593 - acc: 0.4159 - val_loss: 1.6161 - val_acc: 0.4284\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.6043 - acc: 0.4305 - val_loss: 1.6052 - val_acc: 0.4281\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.5490 - acc: 0.4594 - val_loss: 1.5852 - val_acc: 0.4397\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 12s - loss: 1.4992 - acc: 0.4717 - val_loss: 1.5853 - val_acc: 0.4378\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.4427 - acc: 0.4948 - val_loss: 1.5665 - val_acc: 0.4416\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.4141 - acc: 0.5081 - val_loss: 1.5751 - val_acc: 0.4395\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 14s - loss: 1.3626 - acc: 0.5159 - val_loss: 1.5578 - val_acc: 0.4400\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.3170 - acc: 0.5398 - val_loss: 1.5575 - val_acc: 0.4404\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.2762 - acc: 0.5479 - val_loss: 1.5647 - val_acc: 0.4405\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.2540 - acc: 0.5606 - val_loss: 1.5603 - val_acc: 0.4433\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.2102 - acc: 0.5760 - val_loss: 1.5805 - val_acc: 0.4392\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.1892 - acc: 0.5815 - val_loss: 1.5639 - val_acc: 0.4487\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.1342 - acc: 0.6053 - val_loss: 1.5498 - val_acc: 0.4560\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.1179 - acc: 0.6057 - val_loss: 1.5842 - val_acc: 0.4414\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.0845 - acc: 0.6258 - val_loss: 1.5528 - val_acc: 0.4501\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.0411 - acc: 0.6342 - val_loss: 1.5797 - val_acc: 0.4476\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 13s - loss: 1.0207 - acc: 0.6441 - val_loss: 1.5817 - val_acc: 0.4453\n"
     ]
    }
   ],
   "source": [
    "mlp.layers[1].set_weights(autoencoder.layers[2].get_weights())   # Import the encoder weights of autoencoder  \n",
    "mlp.layers[3].set_weights(autoencoder1.layers[2].get_weights())  # Import the encoder weights of autoencoder1\n",
    "\n",
    "\n",
    "history = mlp.fit(train_data, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
